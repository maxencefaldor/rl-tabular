{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7777d36",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c259d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f605",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "Frozen Lake is a good environment to demonstrate reinforcement learning methods in the tabular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b6f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space : \tDiscrete(16)\n",
      "Action space : \t\tDiscrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "print(\"Observation space : \\t{}\".format(env.observation_space))\n",
    "print(\"Action space : \\t\\t{}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a96c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Random agent\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453eae7",
   "metadata": {},
   "source": [
    "A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships. For any policy $\\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:\n",
    "$$v_\\pi(s)=\\sum\\limits_{a}\\pi(a|s) \\sum\\limits_{s', r} p(s',r|s,a) [r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "The value function can be computed by solving this system of linear equations.\n",
    "\n",
    "## Linear Programming\n",
    "As we want to compute the value function for the optimal policy $\\pi^*$ and not any policy $\\pi$, let's rewrite the system of linear equations. The Bellman optimality equation states that\n",
    "$$\\forall s\\in S, v_*(s)=\\max\\limits_{a}\\sum\\limits_{s', r} p(s',r|s,a) [r + \\gamma v_*(s')]$$\n",
    "Therefore\n",
    "$$\\forall (s, a) \\in S\\times A, \\quad v_*(s) \\geq \\sum\\limits_{s', r} p(s',r|s,a) [r + \\gamma v_*(s')]$$\n",
    "$$\\forall (s, a) \\in S\\times A, \\quad v_*(s) - \\gamma \\sum\\limits_{s', r} p(s',r|s,a) v_*(s') \\geq \\sum\\limits_{s', r} p(s',r|s,a) r$$\n",
    "\n",
    "The optimal value function can then be found by solving a linear programming problem:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s} v(s)\\\\\n",
    "s.t. \\ \\forall (s,a)\\in S\\times A, \\quad v(s) - \\gamma \\sum\\limits_{s', r} p(s',r|s,a) v(s') \\geq \\sum\\limits_{s', r} p(s',r|s,a) r\n",
    "\\end{array}\\right.$$\n",
    "Which, finally, is a linear program with $|S|$ variables and $|S||A|$ constraints, if the environment's dynamics are completely known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2f0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:17: OptimizeWarning: Solving system with option 'cholesky':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'cholesky' to False.\n",
      "/home/max/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:17: OptimizeWarning: Solving system with option 'sym_pos':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'sym_pos' to False.\n",
      "/home/max/anaconda3/envs/rl/lib/python3.7/site-packages/scipy/optimize/_linprog_ip.py:117: LinAlgWarning: Ill-conditioned matrix (rcond=2.078e-18): result may not be accurate.\n",
      "  return sp.linalg.solve(M, r, sym_pos=sym_pos)\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "c = np.ones(env.observation_space.n)\n",
    "A_ub = np.zeros((env.observation_space.n*env.action_space.n, env.observation_space.n))\n",
    "b_ub = np.zeros((env.observation_space.n*env.action_space.n,))\n",
    "for (i, (s, a)) in enumerate(product(range(env.observation_space.n), range(env.action_space.n))):\n",
    "    r = sum([p*r for p, s_, r, d in env.P[s][a]])\n",
    "    b_ub[i] = r\n",
    "    \n",
    "    A_ub[i][s] = 1\n",
    "    for p, s_, r, d in env.P[s][a]:\n",
    "        A_ub[i][s_] += -gamma*p\n",
    "A_ub = -A_ub\n",
    "b_ub = -b_ub\n",
    "\n",
    "res = linprog(c, A_ub=A_ub, b_ub=b_ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988d1e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "[[8.23529412e-01 8.23529412e-01 8.23529412e-01 8.23529412e-01]\n",
      " [8.23529412e-01 1.26967246e-11 5.29411765e-01 5.56204911e-12]\n",
      " [8.23529412e-01 8.23529412e-01 7.64705882e-01 3.57139713e-12]\n",
      " [1.67169236e-11 8.82352941e-01 9.41176471e-01 2.97834464e-13]]\n"
     ]
    }
   ],
   "source": [
    "print(res[\"message\"])\n",
    "V = res[\"x\"].reshape(4, 4)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9bd95",
   "metadata": {},
   "source": [
    "# Policy Iteration and Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ef0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, epsilon, V=np.zeros(env.observation_space.n), pi=np.zeros(env.observation_space.n)):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            V[s] = sum([p*(r + gamma*V[s_]) for p, s_, r, d in env.P[s][pi[s]]])\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    policy_stable = True\n",
    "    for s in range(env.observation_space.n):\n",
    "        old_action = pi[s]\n",
    "        pi[s] = np.argmax([sum([p*(r + gamma*V[s_]) for p, s_, r, d in env.P[s][a]]) for a in range(env.action_space.n)])\n",
    "        if old_action != pi[s]:\n",
    "            policy_stable = False\n",
    "    if policy_stable == True:\n",
    "        return V, pi\n",
    "    else:\n",
    "        return policy_iteration(env, gamma=gamma, epsilon=epsilon, V=V, pi=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7aec7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, epsilon):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    pi = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            update = [sum([p*(r + gamma*V[s_]) for p, s_, r, d in env.P[s][a]]) for a in env.P[s].keys()]\n",
    "            V[s] = max(update)\n",
    "            pi[s] = np.argmax(update)\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        if delta < epsilon:\n",
    "            return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f33e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82188098 0.82133959 0.82096328 0.8207718 ]\n",
      " [0.82204676 0.         0.52828734 0.        ]\n",
      " [0.82230639 0.82263933 0.76392582 0.        ]\n",
      " [0.         0.88173433 0.9408617  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V, pi = policy_iteration(env, gamma=1, epsilon=0.0001)\n",
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "159331f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82182145 0.82126109 0.82087163 0.82067347]\n",
      " [0.82199325 0.         0.52824715 0.        ]\n",
      " [0.82226231 0.82260733 0.76389785 0.        ]\n",
      " [0.         0.88171208 0.94085038 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V, pi = value_iteration(env, gamma=1, epsilon=0.0001)\n",
    "print(V.reshape(4, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
